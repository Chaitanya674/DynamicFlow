import json
from prompts import *
from state import AgentState
from langgraph.graph import StateGraph, END
from tools import llm, llm_worker, write_file, read_file, run_shell_command
import json

# --- 1. ARCHITECT ---
def architect_node(state: AgentState):
    print(f"DEBUG STATE KEYS: {list(state.keys())}") 
    
    print(f"\nüèóÔ∏è  [Architect] Designing {state['project_root']}...")
    
    msg = architect_prompt.format(
        project_root=state["project_root"], 
        user_query=state["requirements"],
    )
    response = llm.invoke(msg)
    print(response.content)
    return {"architecture": response.content}

# --- 2. PLANNER ---
def planner_node(state: AgentState):
    print("\nüìÖ [Planner] Creating task list...")

    if state.get("task_queue") or state.get("completed_tasks"):
        print("   -> Skipping planning (tasks already exist or completed).")
        return {}

    msg = planner_prompt.format(
        project_root=state["project_root"],
        architecture=state["architecture"]
    )
    response = llm.invoke(msg)
    
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        tasks = json.loads(content)
        print(tasks)
        # Ensure it's a list
        if isinstance(tasks, dict) and "tasks" in tasks: tasks = tasks["tasks"]
        return {"task_queue": tasks}
    except Exception as e:
        print(f"Error parsing plan: {e}")
        return {"task_queue": []}

# --- 3. ORCHESTRATOR (The Decision Maker) ---
def orchestrator_node(state: AgentState):
    """
    The Orchestrator node itself acts as a router. 
    It doesn't modify the state, but ensures we pause here 
    before the Conditional Edge makes a decision.
    """
    queue = state.get("task_queue", [])
    status = state.get("test_status", "pending")

    if queue:
        print(f"\nüëÆ [Orchestrator] {len(queue)} tasks pending. Next: {queue[0]['assigned_agent']}...")
    elif status == "pending":
        print("\nüëÆ [Orchestrator] All tasks done. Moving to Testing...")
    elif status == "failed":
        print("\nüëÆ [Orchestrator] Tests failed. Activating Debugger...")
    
    return {}

# Helper function to execute tool calls
def execute_tools(ai_msg):
    """Manually executes tool calls generated by the LLM."""
    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
        for tool_call in ai_msg.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            # Map tool names to actual functions
            tools_map = {
                "write_file": write_file,
                "read_file": read_file,
                "run_shell_command": run_shell_command
            }
            
            selected_tool = tools_map.get(tool_name)
            if selected_tool:
                print(f"   üõ†Ô∏è  Executing Tool: {tool_name} with args: {tool_args}")
                selected_tool.invoke(tool_args)

# --- 4. BACKEND AGENT ---
def backend_agent_node(state: AgentState):
    task = state["task_queue"][0]
    print(f"\n‚öôÔ∏è  [Backend] Working on: {task['description']}")
    
    msg = backend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)

    execute_tools(result)
    
    return {
        "completed_tasks": [task],
        "task_queue": state["task_queue"][1:]
    }

# --- 5. FRONTEND AGENT ---
def frontend_agent_node(state: AgentState):
    task = state["task_queue"][0]
    print(f"\nüé® [Frontend] Working on: {task['description']}")
    
    msg = frontend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)

    execute_tools(result)
    
    return {
        "completed_tasks": [task],
        "task_queue": state["task_queue"][1:]
    }

# --- 6. TESTER ---
def tester_node(state: AgentState):
    print("\nüß™ [Tester] Verifying application...")
    msg = tester_prompt_template.format(project_root=state["project_root"])
    

    response = llm_worker.invoke(msg)

    execute_tools(response)

    logs = response.content
    
    status = "passed"
    if any(err in logs for err in ["Error", "Traceback", "Failed"]):
        status = "failed"
        print("   -> ‚ùå Tests Failed")
    else:
        print("   -> ‚úÖ Tests Passed")

    return {"test_logs": logs, "test_status": status}

def debugger_node(state: AgentState):
    print("\nüêû [Debugger] Analyzing errors and creating fix...")
    msg = debugger_prompt.format(test_logs=state["test_logs"])
    response = llm.invoke(msg)
    
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        fix_task = json.loads(content)
        print(f"   -> Created Fix Task: {fix_task['description']}")

        return {
            "task_queue": [fix_task] + state["task_queue"],
            "iteration_count": state["iteration_count"] + 1,
            "test_status": "pending" 
        }
    except:
        return {"iteration_count": state["iteration_count"] + 1}

def select_next_step(state: AgentState):
    """
    This function determines which node to visit next 
    based on the current state (queue, test status, etc.).
    """
    queue = state.get("task_queue", [])

    if queue:
        next_task = queue[0]
        agent_type = next_task.get("assigned_agent", "backend")
        
        if agent_type == "frontend":
            return "frontend"
        elif agent_type == "backend":
            return "backend"
        else:
            return "backend"

    test_status = state.get("test_status", "pending")
    if test_status == "pending":
        return "tester"

    if test_status == "failed":
        iteration = state.get("iteration_count", 0)
        if iteration >= 5:
            print("\n‚ùå [Orchestrator] Max retries (5) reached. Stopping workflow.")
            return END 
        
        return "debugger"

    if test_status == "passed":
        print("\n‚úÖ [Orchestrator] Workflow completed successfully!")
        return END

    return END

workflow = StateGraph(AgentState)

# Add Nodes
workflow.add_node("architect", architect_node)
workflow.add_node("planner", planner_node)
workflow.add_node("orchestrator", orchestrator_node)
workflow.add_node("backend", backend_agent_node)
workflow.add_node("frontend", frontend_agent_node)
workflow.add_node("tester", tester_node)
workflow.add_node("debugger", debugger_node)

workflow.set_entry_point("architect")

workflow.add_edge("architect", "planner")
workflow.add_edge("planner", "orchestrator")

workflow.add_conditional_edges(
    "orchestrator",   
    select_next_step,    
    {
        "backend": "backend",
        "frontend": "frontend",
        "tester": "tester",
        "debugger": "debugger",
        END: END
    }
)

workflow.add_edge("backend", "orchestrator")
workflow.add_edge("frontend", "orchestrator")
workflow.add_edge("debugger", "orchestrator") 
workflow.add_edge("tester", "orchestrator") 
# Compile
app = workflow.compile()