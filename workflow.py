import json
from prompts import *
from state import AgentState
from langgraph.graph import StateGraph, END
from tools import llm, llm_worker, write_file, read_file, run_shell_command
import json

# --- 1. ARCHITECT ---
def architect_node(state: AgentState):
    # DEBUG: Print all keys in the state to see what is missing
    print(f"DEBUG STATE KEYS: {list(state.keys())}") 
    
    print(f"\nðŸ—ï¸  [Architect] Designing {state['project_root']}...")
    
    msg = architect_prompt.format(
        project_root=state["project_root"], 
        user_query=state["requirements"],
        requirements=state["requirements"]  # Fix for KeyError
    )
    response = llm.invoke(msg)
    print(response.content)
    return {"architecture": response.content}

# --- 2. PLANNER ---
def planner_node(state: AgentState):
    print("\nðŸ“… [Planner] Creating task list...")
    # Only plan if we have no tasks and haven't started (prevent infinite replanning loops)
    if state.get("task_queue") or state.get("completed_tasks"):
        print("   -> Skipping planning (tasks already exist or completed).")
        return {}

    msg = planner_prompt.format(
        project_root=state["project_root"],
        architecture=state["architecture"]
    )
    response = llm.invoke(msg)
    
    # Parsing logic (simplified for brevity)
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        tasks = json.loads(content)
        print(tasks)
        # Ensure it's a list
        if isinstance(tasks, dict) and "tasks" in tasks: tasks = tasks["tasks"]
        return {"task_queue": tasks}
    except Exception as e:
        print(f"Error parsing plan: {e}")
        return {"task_queue": []}

# --- 3. ORCHESTRATOR (The Decision Maker) ---
def orchestrator_node(state: AgentState):
    """
    The Orchestrator node itself acts as a router. 
    It doesn't modify the state, but ensures we pause here 
    before the Conditional Edge makes a decision.
    """
    queue = state.get("task_queue", [])
    status = state.get("test_status", "pending")
    
    # Logging for visibility
    if queue:
        print(f"\nðŸ‘® [Orchestrator] {len(queue)} tasks pending. Next: {queue[0]['assigned_agent']}...")
    elif status == "pending":
        print("\nðŸ‘® [Orchestrator] All tasks done. Moving to Testing...")
    elif status == "failed":
        print("\nðŸ‘® [Orchestrator] Tests failed. Activating Debugger...")
    
    return {}

# Helper function to execute tool calls
def execute_tools(ai_msg):
    """Manually executes tool calls generated by the LLM."""
    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
        for tool_call in ai_msg.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            # Map tool names to actual functions
            tools_map = {
                "write_file": write_file,
                "read_file": read_file,
                "run_shell_command": run_shell_command
            }
            
            selected_tool = tools_map.get(tool_name)
            if selected_tool:
                print(f"   ðŸ› ï¸  Executing Tool: {tool_name} with args: {tool_args}")
                selected_tool.invoke(tool_args)

# --- 4. BACKEND AGENT ---
def backend_agent_node(state: AgentState):
    task = state["task_queue"][0]
    print(f"\nâš™ï¸  [Backend] Working on: {task['description']}")
    
    msg = backend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)
    
    # NEW: Execute the tool calls
    execute_tools(result)
    
    return {
        "completed_tasks": [task],
        "task_queue": state["task_queue"][1:]
    }

# --- 5. FRONTEND AGENT ---
def frontend_agent_node(state: AgentState):
    task = state["task_queue"][0]
    print(f"\nðŸŽ¨ [Frontend] Working on: {task['description']}")
    
    msg = frontend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)
    
    # NEW: Execute the tool calls
    execute_tools(result)
    
    return {
        "completed_tasks": [task],
        "task_queue": state["task_queue"][1:]
    }

# --- 6. TESTER ---
def tester_node(state: AgentState):
    print("\nðŸ§ª [Tester] Verifying application...")
    msg = tester_prompt_template.format(project_root=state["project_root"])
    
    # Use llm_worker to ensure tools are available
    response = llm_worker.invoke(msg)
    
    # NEW: Execute the tool calls (e.g., run_shell_command)
    execute_tools(response)
    
    # Logic to capture logs from the response content or tool output
    logs = response.content
    
    status = "passed"
    if any(err in logs for err in ["Error", "Traceback", "Failed"]):
        status = "failed"
        print("   -> âŒ Tests Failed")
    else:
        print("   -> âœ… Tests Passed")

    return {"test_logs": logs, "test_status": status}

def debugger_node(state: AgentState):
    print("\nðŸž [Debugger] Analyzing errors and creating fix...")
    msg = debugger_prompt.format(test_logs=state["test_logs"])
    response = llm.invoke(msg)
    
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        fix_task = json.loads(content)
        print(f"   -> Created Fix Task: {fix_task['description']}")
        
        # Add the fix task to the FRONT of the queue to be executed immediately
        return {
            "task_queue": [fix_task] + state["task_queue"],
            "iteration_count": state["iteration_count"] + 1,
            "test_status": "pending" # Reset status to force re-test
        }
    except:
        return {"iteration_count": state["iteration_count"] + 1}

def select_next_step(state: AgentState):
    """
    This function determines which node to visit next 
    based on the current state (queue, test status, etc.).
    """
    queue = state.get("task_queue", [])
    
    # 1. If there are tasks in the queue, route to the assigned agent
    if queue:
        next_task = queue[0]
        agent_type = next_task.get("assigned_agent", "backend")
        
        if agent_type == "frontend":
            return "frontend"
        elif agent_type == "backend":
            return "backend"
        else:
            # Fallback if unknown agent
            return "backend"
    
    # 2. If queue is empty, check if we need to run tests
    test_status = state.get("test_status", "pending")
    if test_status == "pending":
        return "tester"
    
    # 3. If tests failed, route to debugger (unless we've retried too many times)
    if test_status == "failed":
        iteration = state.get("iteration_count", 0)
        if iteration >= 5:
            print("\nâŒ [Orchestrator] Max retries (5) reached. Stopping workflow.")
            return END # Stop the graph
        
        return "debugger"
        
    # 4. If tests passed and queue is empty, we are done!
    if test_status == "passed":
        print("\nâœ… [Orchestrator] Workflow completed successfully!")
        return END

    return END

# --- Workflow Definition ---
workflow = StateGraph(AgentState)

# Add Nodes
workflow.add_node("architect", architect_node)
workflow.add_node("planner", planner_node)
workflow.add_node("orchestrator", orchestrator_node)
workflow.add_node("backend", backend_agent_node)
workflow.add_node("frontend", frontend_agent_node)
workflow.add_node("tester", tester_node)
workflow.add_node("debugger", debugger_node)

# Set Entry Point
workflow.set_entry_point("architect")

# Linear Flow: Architect -> Planner -> Orchestrator
workflow.add_edge("architect", "planner")
workflow.add_edge("planner", "orchestrator")

# Conditional Flow from Orchestrator
workflow.add_conditional_edges(
    "orchestrator",       # Start at Orchestrator
    select_next_step,     # Run this function to decide where to go
    {
        "backend": "backend",
        "frontend": "frontend",
        "tester": "tester",
        "debugger": "debugger",
        END: END
    }
)

# Return loops
workflow.add_edge("backend", "orchestrator")
workflow.add_edge("frontend", "orchestrator")
workflow.add_edge("debugger", "orchestrator") # After creating fix, go back to dispatch it
workflow.add_edge("tester", "orchestrator")   # After testing, go back to let Orchestrator decide (End or Debug)

# Compile
app = workflow.compile()