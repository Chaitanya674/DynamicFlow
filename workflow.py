import json
from prompts import *
from state import AgentState, WorkerState
from langgraph.graph import StateGraph, END
from langgraph.types import Send
from tools import llm, llm_worker, write_file, read_file, run_shell_command
import json

# --- 1. ARCHITECT ---
def architect_node(state: AgentState):
    print(f"DEBUG STATE KEYS: {list(state.keys())}") 
    
    print(f"\nüèóÔ∏è  [Architect] Designing {state['project_root']}...")
    
    msg = architect_prompt.format(
        project_root=state["project_root"], 
        user_query=state["requirements"],
    )
    response = llm.invoke(msg)
    print(response.content)
    return {"architecture": response.content}

# --- 2. PLANNER ---
def planner_node(state: AgentState):
    print("\nüìÖ [Planner] Creating task list...")

    if state.get("task_queue") or state.get("completed_tasks"):
        print("   -> Skipping planning (tasks already exist or completed).")
        return {}

    msg = planner_prompt.format(
        project_root=state["project_root"],
        architecture=state["architecture"]
    )
    response = llm.invoke(msg)
    
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        tasks = json.loads(content)
        print(tasks)
        # Ensure it's a list
        if isinstance(tasks, dict) and "tasks" in tasks: tasks = tasks["tasks"]
        return {"task_queue": tasks}
    except Exception as e:
        print(f"Error parsing plan: {e}")
        return {"task_queue": []}

# --- 3. ORCHESTRATOR (The Decision Maker) ---
def orchestrator_node(state: AgentState):
    """
    The Orchestrator node itself acts as a router. 
    It doesn't modify the state, but ensures we pause here 
    before the Conditional Edge makes a decision.
    """
    queue = state.get("task_queue", [])
    status = state.get("test_status", "pending")

    if queue:
        print(f"\nüëÆ [Orchestrator] {len(queue)} tasks pending. Next: {queue[0]['assigned_agent']}...")
    elif status == "pending":
        print("\nüëÆ [Orchestrator] All tasks done. Moving to Testing...")
    elif status == "failed":
        print("\nüëÆ [Orchestrator] Tests failed. Activating Debugger...")
    
    return {}

# Helper function to execute tool calls
def execute_tools(ai_msg):
    """Manually executes tool calls generated by the LLM."""
    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
        for tool_call in ai_msg.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            # Map tool names to actual functions
            tools_map = {
                "write_file": write_file,
                "read_file": read_file,
                "run_shell_command": run_shell_command
            }
            
            selected_tool = tools_map.get(tool_name)
            if selected_tool:
                print(f"   üõ†Ô∏è  Executing Tool: {tool_name} with args: {tool_args}")
                selected_tool.invoke(tool_args)

# --- 4. BACKEND WORKER (Parallel Execution) ---
def backend_worker(state: WorkerState):
    """Worker that executes a single backend task in parallel"""
    task = state["task"]
    print(f"\n‚öôÔ∏è  [Backend Worker] Working on: {task['description'][:60]}...")
    
    msg = backend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)

    execute_tools(result)
    
    # Mark task as completed
    task_completed = task.copy()
    task_completed["status"] = "completed"
    
    # Return to be merged with main state via operator.add
    return {"completed_tasks": [task_completed]}

# --- 5. FRONTEND WORKER (Parallel Execution) ---
def frontend_worker(state: WorkerState):
    """Worker that executes a single frontend task in parallel"""
    task = state["task"]
    print(f"\nüé® [Frontend Worker] Working on: {task['description'][:60]}...")
    
    msg = frontend_prompt_template.format(
        task_description=task["description"],
        project_root=state["project_root"]
    )
    result = llm_worker.invoke(msg)

    execute_tools(result)
    
    # Mark task as completed
    task_completed = task.copy()
    task_completed["status"] = "completed"
    
    # Return to be merged with main state via operator.add
    return {"completed_tasks": [task_completed]}

# --- 6. TESTER ---
def tester_node(state: AgentState):
    print("\nüß™ [Tester] Verifying application...")
    msg = tester_prompt_template.format(project_root=state["project_root"])
    

    response = llm_worker.invoke(msg)

    execute_tools(response)

    logs = response.content
    
    status = "passed"
    if any(err in logs for err in ["Error", "Traceback", "Failed"]):
        status = "failed"
        print("   -> ‚ùå Tests Failed")
    else:
        print("   -> ‚úÖ Tests Passed")

    return {"test_logs": logs, "test_status": status}

# --- 7. SYNTHESIZER (Collect Results from Parallel Workers) ---
def synthesizer_node(state: AgentState):
    """Synthesizes all completed tasks into a final report"""
    print("\nüîó [Synthesizer] Compiling results from all workers...")
    
    completed_tasks = state.get("completed_tasks", [])
    
    if not completed_tasks:
        print("   -> No tasks completed yet.")
        return {"final_report": "No tasks completed"}
    
    # Create a summary report
    report_sections = []
    backend_tasks = [t for t in completed_tasks if t.get("assigned_agent") == "backend"]
    frontend_tasks = [t for t in completed_tasks if t.get("assigned_agent") == "frontend"]
    
    report_sections.append(f"‚úÖ Total Tasks Completed: {len(completed_tasks)}")
    report_sections.append(f"   - Backend: {len(backend_tasks)} tasks")
    report_sections.append(f"   - Frontend: {len(frontend_tasks)} tasks")
    report_sections.append("\nCompleted Tasks:")
    
    for i, task in enumerate(completed_tasks, 1):
        report_sections.append(f"{i}. [{task.get('assigned_agent', 'unknown').upper()}] {task.get('description', 'No description')[:80]}...")
    
    final_report = "\n".join(report_sections)
    print(final_report)
    
    return {"final_report": final_report, "task_queue": []}

# --- 8. DEBUGGER ---
def debugger_node(state: AgentState):
    print("\nüêû [Debugger] Analyzing errors and creating fix...")
    msg = debugger_prompt.format(test_logs=state["test_logs"])
    response = llm.invoke(msg)
    
    try:
        content = response.content.replace("```json", "").replace("```", "").strip()
        fix_task = json.loads(content)
        print(f"   -> Created Fix Task: {fix_task['description']}")

        return {
            "task_queue": [fix_task],
            "iteration_count": state["iteration_count"] + 1,
            "test_status": "pending",
            "completed_tasks": []  # Reset for re-execution
        }
    except:
        return {"iteration_count": state["iteration_count"] + 1}

# --- 9. ASSIGN WORKERS (Send API for Parallel Distribution) ---
def assign_workers(state: AgentState):
    """
    Conditional edge function that uses Send() to distribute tasks to workers in parallel.
    Each Send() creates a worker node execution with its own WorkerState.
    """
    queue = state.get("task_queue", [])
    
    if not queue:
        # No tasks to assign, move to synthesizer
        print("\nüëÆ [Orchestrator] No tasks in queue, moving to synthesis...")
        return "synthesizer"
    
    print(f"\nüëÆ [Orchestrator] Distributing {len(queue)} tasks to workers in parallel...")
    
    # Create a Send() for each task to enable parallel execution
    sends = []
    for task in queue:
        agent_type = task.get("assigned_agent", "backend")
        
        # Create worker state with task and project_root
        worker_state = {
            "task": task,
            "project_root": state["project_root"]
        }
        
        if agent_type == "frontend":
            sends.append(Send("frontend_worker", worker_state))
        else:
            sends.append(Send("backend_worker", worker_state))
    
    return sends

# --- 10. TEST DECISION ---
def test_decision(state: AgentState):
    """
    Determines next step after testing.
    """
    test_status = state.get("test_status", "pending")

    if test_status == "failed":
        iteration = state.get("iteration_count", 0)
        if iteration >= 5:
            print("\n‚ùå [Test Decision] Max retries (5) reached. Stopping workflow.")
            return END 
        
        return "debugger"

    if test_status == "passed":
        print("\n‚úÖ [Test Decision] All tests passed! Workflow completed successfully!")
        return END

    return END

# =============================================================================
# BUILD WORKFLOW GRAPH WITH PARALLEL WORKERS
# =============================================================================

workflow = StateGraph(AgentState)

# Add Nodes
workflow.add_node("architect", architect_node)
workflow.add_node("planner", planner_node)
workflow.add_node("orchestrator", orchestrator_node)

# Parallel workers (use WorkerState)
workflow.add_node("backend_worker", backend_worker)
workflow.add_node("frontend_worker", frontend_worker)

# Synthesis and testing
workflow.add_node("synthesizer", synthesizer_node)
workflow.add_node("tester", tester_node)
workflow.add_node("debugger", debugger_node)

# Entry point
workflow.set_entry_point("architect")

# Planning phase (sequential)
workflow.add_edge("architect", "planner")
workflow.add_edge("planner", "orchestrator")

# Parallel task distribution using Send() API
workflow.add_conditional_edges(
    "orchestrator",
    assign_workers,  # Returns list of Send() or "synthesizer"
    ["backend_worker", "frontend_worker", "synthesizer"]
)

# Both workers flow to synthesizer
workflow.add_edge("backend_worker", "synthesizer")
workflow.add_edge("frontend_worker", "synthesizer")

# Testing phase
workflow.add_edge("synthesizer", "tester")

# Test result decision
workflow.add_conditional_edges(
    "tester",
    test_decision,
    {"debugger": "debugger", END: END}
)

# Debugger re-plans and goes back to orchestrator
workflow.add_edge("debugger", "orchestrator")

# Compile the workflow
app = workflow.compile()